{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4694 docs, 200 queries.\n",
      "test: 4677 docs, 200 queries.\n",
      "25 features\n",
      "un-normalized train: 70.080336425764 <101.65279842486342>\n",
      "un-normalized test: 75.07174581020573 <156.60887376760795>\n",
      "normalized train: 5.180490685311132e-07 <0.7804636401064724>\n",
      "normalized test: 1.3643793263153821e-06 <0.7887188357739844>\n",
      "train: 2672 docs, 632 queries.\n",
      "test: 4298 docs, 635 queries.\n",
      "25 features\n",
      "un-normalized train: 100.59542148518399 <112.00393006016057>\n",
      "un-normalized test: 107.18769941395159 <118.13138269196044>\n",
      "normalized train: 7.412363746142352e-08 <0.7544854415648509>\n",
      "normalized test: -2.8577670556600825e-08 <0.7641385980060638>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Mmetrics import *\n",
    "\n",
    "import LTR\n",
    "import datautil\n",
    "import permutationgraph\n",
    "import DTR\n",
    "import EEL\n",
    "import PPG\n",
    "import PL\n",
    "\n",
    "ds2020, _ = datautil.load_data(2020, verbose=True)\n",
    "ds2019, _ = datautil.load_data(2019, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a LTR model (MSE)\n",
    "\n",
    "Set `y_pred` using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrmodel = LTR.MSE_model(layers=[ds2020.trfm.shape[1], 256, 256, 1], lr=0.001, optimizer=torch.optim.Adam, dropout=0.1)\n",
    "ltrmodel.fit(ds2020, epochs=10, batch_size=100, verbose=True)\n",
    "y_pred2020 = ltrmodel.predict(ds2020.tefm, ds2020.tedlr)\n",
    "\n",
    "ltrmodel = LTR.MSE_model(layers=[ds2019.trfm.shape[1], 256, 256, 1], lr=0.001, optimizer=torch.optim.Adam, dropout=0.1)\n",
    "ltrmodel.fit(ds2019, epochs=10, batch_size=100, verbose=True)\n",
    "y_pred2019 = ltrmodel.predict(ds2019.tefm, ds2019.tedlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Query Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `QueryLearner` class for one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 6\n",
    "sessions = 1\n",
    "s, e = ds2020.tedlr[qid:qid+2]\n",
    "y_pred = y_pred2020[s:e]\n",
    "sorted_docs = y_pred.argsort()[::-1]\n",
    "g = ds2020.teg[s:e]\n",
    "\n",
    "\n",
    "# objective_ins = DTR.DTR(y_pred = y_pred, g = g, dlr = None, exposure = np.array([1./np.log2(2+i) for i in range(1,1000)]), method='query_ratio')\n",
    "\n",
    "y, g, sorted_docs, dlr = EEL.copy_sessions(y=y_pred, g=g, sorted=sorted_docs, sessions=sessions)\n",
    "\n",
    "print(sorted_docs)\n",
    "print(dlr)\n",
    "objective_ins = EEL.EEL(y_pred = y, g = g, dlr = dlr, grade_levels=5, exposure = np.array([1./np.log2(2+i) for i in range(1,1000)]))\n",
    "\n",
    "n = y.shape[0]\n",
    "# learner = PPG.Learner(0.5 * np.triu(np.ones((n,n)), 1), samples_cnt=16, \n",
    "#                         objective_ins=objective_ins, sorted_docs=sorted_docs, dlr=dlr, intra=g, inter=np.repeat(dlr[:-1], np.diff(dlr)))\n",
    "# learner = permutationgraph.QueryLearner(objective_ins, sorted_docs = sorted_docs, intra = g)\n",
    "learner = PL.Learner(logits=y, samples_cnt=256, objective_ins=objective_ins)\n",
    "learner.fit(50, 0.1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = PPG.Learner(0.5 * np.triu(np.ones((n,n)), 1), samples_cnt=256, \n",
    "                        objective_ins=objective_ins, sorted_docs=sorted_docs, dlr=dlr, intra=g, inter=np.repeat(dlr[:-1], np.diff(dlr)))\n",
    "learner.fit(50, 0.1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `QueryLearner` class for all queries, using `learn_all_query` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learn_all_query(y_pred, g, dlr, exposure, epochs, lr, learner_cls, objective, objective_args=None):\n",
    "    y_rerank = []\n",
    "    sorted_docs = []\n",
    "    min_vals = []\n",
    "    \n",
    "    # for qid in trange(dlr.shape[0] - 1, leave=False):\n",
    "    for qid in range(dlr.shape[0] - 1):\n",
    "        s, e = dlr[qid:qid+2]\n",
    "        if objective == 'DTR':\n",
    "            objective_ins = DTR.DTR(y_pred = y_pred[s:e], g = g[s:e], dlr = None, exposure = exposure, method='query_ratio')\n",
    "        elif objective == 'EEL':\n",
    "            objective_ins = EEL.EEL(y_pred = y_pred[s:e], g = g[s:e], dlr = np.array([0,e-s]), exposure = exposure, **objective_args)\n",
    "\n",
    "        learner = learner_cls(objective_ins, sorted_docs = y_pred[s:e].argsort()[::-1], intra = g[s:e])\n",
    "        vals = learner.fit(epochs, lr, verbose=False)\n",
    "\n",
    "        scores = np.arange(len(learner.sorted_docs), 0, -1)\n",
    "        y_rerank.append(scores[learner.sorted_docs])\n",
    "        sorted_docs.append(learner.sorted_docs)\n",
    "        \n",
    "        vals = np.array(vals)\n",
    "        min_vals.append(vals.min())\n",
    "\n",
    "    # print(ndcg_dtr(exposure, lv, np.concatenate(y_rerank), dlr, g, query_counts))\n",
    "    return np.concatenate(y_rerank), np.concatenate(sorted_docs), np.array(min_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TREC 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective='EEL'\n",
    "objective_args = {'grade_levels':2}\n",
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "y_rerank2020, sorted2020, min_vals = learn_all_query(  y_pred2020, ds2020.teg, ds2020.tedlr, \n",
    "                                            exposure = exposure2020,\n",
    "                                            epochs=20, lr=0.3, \n",
    "                                            learner_cls=permutationgraph.QueryLearner,\n",
    "                                            objective=objective,\n",
    "                                            objective_args = objective_args)\n",
    "print(f'{len(min_vals[min_vals>0])} valid queries. --> average: {min_vals[min_vals>0].mean()}')\n",
    "\n",
    "if objective == 'DTR':\n",
    "    print(DTR.ndcg_dtr(exposure2020, ds2020.telv, y_rerank2020, ds2020.tedlr, ds2020.teg, ds2020.query_seq))\n",
    "elif objective == 'EEL':\n",
    "    eel = EEL.EEL(y_pred=ds2020.telv, g=ds2020.teg, dlr=ds2020.tedlr, exposure=exposure2020, grade_levels=2)\n",
    "    print(eel.eval(sorted2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with true labels instead of LTR output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective='EEL'\n",
    "objective_args = {'grade_levels':2}\n",
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "y_rerank2020, sorted2020, min_vals = learn_all_query(  ds2020.telv, ds2020.teg, ds2020.tedlr, \n",
    "                                            exposure = exposure2020,\n",
    "                                            epochs=5, lr=0.3, \n",
    "                                            learner_cls=permutationgraph.QueryLearner,\n",
    "                                            objective=objective,\n",
    "                                            objective_args = objective_args)\n",
    "print(f'{len(min_vals[min_vals>0])} valid queries. --> average: {min_vals[min_vals>0].mean()}')\n",
    "\n",
    "if objective == 'DTR':\n",
    "    print(DTR.ndcg_dtr(exposure2020, ds2020.telv, y_rerank2020, ds2020.tedlr, ds2020.teg, ds2020.query_seq))\n",
    "elif objective == 'EEL':\n",
    "    eel = EEL.EEL(y_pred=ds2020.telv, g=ds2020.teg, dlr=ds2020.tedlr, exposure=exposure2020, grade_levels=2)\n",
    "    print(eel.eval(sorted2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epochs, lr):\n",
    "    exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "    y_rerank2020, sorted2020, min_vals = learn_all_query(  ds2020.telv, ds2020.teg, ds2020.tedlr, \n",
    "                                            exposure = exposure2020,\n",
    "                                            epochs=epochs, lr=lr, \n",
    "                                            learner_cls=permutationgraph.QueryLearner,\n",
    "                                            objective=objective,\n",
    "                                            objective_args = {'grade_levels':2})\n",
    "    eel = EEL.EEL(y_pred=ds2020.telv, g=ds2020.teg, dlr=ds2020.tedlr, exposure=exposure2020, grade_levels=2)\n",
    "    return eel.eval(sorted2020)\n",
    "\n",
    "for epochs in [2,5,10,20,40,100]:\n",
    "    print(epochs)\n",
    "    for lr in [0,0.05,0.1,0.3]:\n",
    "        eel = []\n",
    "        for i in range(8):\n",
    "            eel.append(test(5,lr))\n",
    "        eel = np.array(eel)\n",
    "        print([lr, eel.mean(), eel.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lr in [0,0.1,0.3,0.5]:\n",
    "    eel = []\n",
    "    for i in range(8):\n",
    "        eel.append(test(5,lr))\n",
    "    eel = np.array(eel)\n",
    "    print([lr, eel.mean(), eel.std()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TREC 2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure2019 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2019.tedlr).max()+2)])\n",
    "y_rerank2019, min_vals = learn_all_query(  y_pred2019, ds2019.teg, ds2019.tedlr, \n",
    "                                            exposure=exposure2019,\n",
    "                                            epochs=50, lr=0.3, \n",
    "                                            learner_cls=permutationgraph.QueryLearner)\n",
    "print(f'{len(min_vals[min_vals>0])} valid queries.')\n",
    "DTR.ndcg_dtr(exposure2019, ds2019.telv, y_rerank2019, ds2019.tedlr, ds2019.teg, ds2019.query_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some checks for weirdness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DTR.ndcg_dtr(exposure2019, ds2019.telv, -y_rerank2019, ds2019.tedlr, ds2019.teg, ds2019.query_seq))\n",
    "print(DTR.ndcg_dtr(exposure2019, ds2019.telv, y_pred2019, ds2019.tedlr, ds2019.teg, ds2019.query_seq))\n",
    "print(DTR.ndcg_dtr(exposure2019, ds2019.telv, -y_pred2019, ds2019.tedlr, ds2019.teg, ds2019.query_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Batch Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `BatchLearner` for a set of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_queries(y, g, dlr, qids):\n",
    "    ys, gs, dlrs = [], [], [0]\n",
    "    for qid in qids:\n",
    "        s, e = dlr[qid:qid+2]\n",
    "        ys.append(y[s:e])\n",
    "        gs.append(g[s:e])\n",
    "        dlrs.append(e-s)\n",
    "    return np.concatenate(ys), np.concatenate(gs), np.cumsum(dlrs)\n",
    "\n",
    "qids = [197,  64]\n",
    "y_pred, gs, dlrs = select_queries(ds2020.telv, ds2020.teg, ds2020.tedlr, qids)\n",
    "objective_ins = DTR.DTR(y_pred = y_pred, g = gs, dlr = dlrs, exposure = np.array([1./np.log2(2+i) for i in range(1,1000)]), method='batch_ratio')\n",
    "\n",
    "ss = []\n",
    "for qid in range(dlrs.shape[0] - 1):\n",
    "    s,e = dlrs[qid:qid+2]\n",
    "    ss.append(y_pred[s:e].argsort()[::-1])\n",
    "sorted_docs = np.concatenate(ss)\n",
    "batch_numbers = np.repeat(dlrs[:-1], np.diff(dlrs))\n",
    "learner = permutationgraph.BatchLearner(objective_ins=objective_ins, sorted_docs=sorted_docs, intra=gs, inter=batch_numbers)\n",
    "learner.fit(50, 0.3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `BatchLearner` class for all queries, using `learn_all_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_group_counts(g, dlr):\n",
    "    groups = np.unique(g)\n",
    "    gcnt = [[] for _ in range(len(groups))]\n",
    "    for qid in range(dlr.shape[0] - 1):\n",
    "        s, e = dlr[qid:qid+2]\n",
    "        for i, group in enumerate(groups):\n",
    "            gcnt[i].append(len(np.where(g[s:e] == group)[0]))\n",
    "    for i, group in enumerate(groups):\n",
    "            gcnt[i] = np.array(gcnt[i])\n",
    "    return groups, gcnt\n",
    "\n",
    "\n",
    "def update_y(y, sorted_docs, dlr, qids):\n",
    "    pos = 0\n",
    "    for qid in qids:\n",
    "        s, e = dlr[qid:qid+2]\n",
    "        scores = np.arange(e-s, 0, -1)\n",
    "        y[s:e] = scores[sorted_docs[pos:pos+e-s]]\n",
    "        pos += e-s\n",
    "def learn_all_batch(y_pred, g, dlr, exposure, epochs, lr, learner_cls):\n",
    "    y_rerank = []\n",
    "    min_vals = []\n",
    "    \n",
    "    groups, gcnt = get_group_counts(g, dlr)\n",
    "    for i, _ in enumerate(groups):\n",
    "            gcnt[i] = gcnt[i].argsort()[:, None]\n",
    "    gcnt = np.concatenate(gcnt, axis=1)\n",
    "    \n",
    "    for qid in range(dlr.shape[0] - 1):\n",
    "        s, e = dlr[qid:qid+2]\n",
    "\n",
    "        sorted_docs = y_pred[s:e].argsort()[::-1]\n",
    "\n",
    "        scores = np.arange(len(sorted_docs), 0, -1)\n",
    "        y_rerank.append(scores[sorted_docs])\n",
    "    y_rerank = np.concatenate(y_rerank)\n",
    "\n",
    "    # for bid in trange(dlr.shape[0] - 1, leave=False):\n",
    "    for bid in range(dlr.shape[0] - 1):\n",
    "        qids = gcnt[bid, :]\n",
    "        # print(qids)\n",
    "\n",
    "        ys, gs, dlrs = select_queries(y_rerank, g, dlr, qids)\n",
    "\n",
    "\n",
    "        objective_ins = DTR.DTR(y_pred = ys, g = gs, dlr = dlrs, exposure = exposure, method='batch_ratio')\n",
    "\n",
    "        ss = []\n",
    "        for qid in range(dlrs.shape[0] - 1):\n",
    "            s,e = dlrs[qid:qid+2]\n",
    "            ss.append(ys[s:e].argsort()[::-1])\n",
    "        sorted_docs = np.concatenate(ss)\n",
    "        batch_numbers = np.repeat(dlrs[:-1], np.diff(dlrs))\n",
    "        learner = learner_cls(objective_ins=objective_ins, sorted_docs=sorted_docs, intra=gs, inter=batch_numbers)\n",
    "\n",
    "        vals = learner.fit(epochs, lr, verbose=False)\n",
    "\n",
    "        update_y(y_rerank, learner.sorted_docs, dlr, qids)\n",
    "        \n",
    "        vals = np.array(vals)\n",
    "        min_vals.append(vals.min())\n",
    "\n",
    "        \n",
    "    return y_rerank, np.array(min_vals)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TREC 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "y_rerank2020, min_vals = learn_all_batch(  y_pred2020, ds2020.teg, ds2020.tedlr, \n",
    "                                            exposure = exposure2020,\n",
    "                                            epochs=200, lr=0.3, \n",
    "                                            learner_cls=permutationgraph.BatchLearner)\n",
    "print(f'{len(min_vals[min_vals>0])} valid queries. --> average: {min_vals[min_vals>0].mean()}')\n",
    "\n",
    "DTR.ndcg_dtr(exposure2020, ds2020.telv, y_rerank2020, ds2020.tedlr, ds2020.teg, ds2020.query_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "y_rerank2020, min_vals = learn_all_batch(  ds2020.telv, ds2020.teg, ds2020.tedlr, \n",
    "                                            exposure = exposure2020,\n",
    "                                            epochs=200, lr=0.3, \n",
    "                                            learner_cls=permutationgraph.BatchLearner)\n",
    "print(f'{len(min_vals[min_vals>0])} valid queries. --> average: {min_vals[min_vals>0].mean()}')\n",
    "\n",
    "DTR.ndcg_dtr(exposure2020, ds2020.telv, y_rerank2020, ds2020.tedlr, ds2020.teg, ds2020.query_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Nonrelevant Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'qid':list(np.repeat(ds2019.teqid, np.diff(ds2019.tedlr))), 'group':list(ds2019.teg), 'label':list(ds2019.telv), 'pred':list(y_pred2019)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "learner = permutationgraph.QueryLearner(np.zeros(4), np.arange(n), None, sorted_docs = None)\n",
    "learner.probs_mat = 0.5 * np.ones([learner.n, learner.n])\n",
    "\n",
    "def per2int(docs):\n",
    "    return (np.array([10**(len(docs)-1-i) for i in range(len(docs))])*docs).sum()\n",
    "freq = defaultdict(lambda:0)\n",
    "iters = 50000\n",
    "for _ in range(iters):\n",
    "    docs, crap = learner.permute()\n",
    "    freq[per2int(docs)] += 2.**(n+1)/iters\n",
    "a = sorted(freq.items(),key=lambda x: x[1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linspan(y_pred, levels):\n",
    "    m,M = y_pred.min()-1e-10, y_pred.max()+1e-10\n",
    "    m = max(m,0)\n",
    "    step = (M - m) / levels\n",
    "    return np.floor((y_pred-m)/step)\n",
    "\n",
    "\n",
    "def disc_target_exposure(y, exposure):\n",
    "    sorted_y = np.sort(y)[::-1]\n",
    "    expo = exposure[:len(y)]\n",
    "    te = []\n",
    "    for g in range(int(y.max()+1)):\n",
    "        te.append(np.mean(expo[sorted_y==g]))\n",
    "    return np.array(te)\n",
    "y_pred = np.random.rand(20)\n",
    "\n",
    "\n",
    "exposure = np.array([1./np.log2(2+i) for i in range(1,1000+2)])\n",
    "\n",
    "print(y_pred)\n",
    "a = linspan(y_pred, 5)\n",
    "b = disc_target_exposure(linspan(y_pred, 5), exposure)\n",
    "[a,b[a.astype(int)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_edges(args):\n",
    "    edges = 0\n",
    "    for i in range(len(args)):\n",
    "        for j in range(i+1, len(args)):\n",
    "            if args[i] > args[j]:\n",
    "                edges += 1\n",
    "    return edges\n",
    "\n",
    "def sample(PPG):\n",
    "    n = PPG.shape[0]\n",
    "    if n <= 1:\n",
    "        return np.arange(n)\n",
    "    selected = np.random.binomial(1,PPG)\n",
    "    positions = np.arange(n) + selected.sum(1) - selected.sum(0)\n",
    "    # print(positions)\n",
    "    empty_positions = []\n",
    "    for i in range(n):\n",
    "        shared_i_s = np.where(positions == i)[0]\n",
    "        if len(shared_i_s) <= 1:\n",
    "            if len(shared_i_s) == 0:\n",
    "                empty_positions.append(i)\n",
    "            continue\n",
    "        chosen_i = np.random.choice(shared_i_s)\n",
    "        for j in shared_i_s:\n",
    "            if j == chosen_i:\n",
    "                continue\n",
    "            positions[j] = -1\n",
    "    remaining = np.where(positions == -1)[0]\n",
    "    # print(remaining)\n",
    "    if len(remaining) > 0:\n",
    "        PPG2 = PPG[remaining,:][:,remaining]\n",
    "        positions2 = sample(PPG2)\n",
    "        positions[remaining] = np.array(empty_positions)[positions2]\n",
    "    return positions\n",
    "\n",
    "\n",
    "n = 5\n",
    "prob = 0.3\n",
    "PPG = prob * np.triu(np.ones((n,n)), 1)\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "freq = defaultdict(lambda:defaultdict(lambda:0))\n",
    "\n",
    "iters = 100000\n",
    "for i in trange(iters):\n",
    "    positions = sample(PPG)\n",
    "    edges = get_edges(positions)\n",
    "    freq[edges][str(positions)] += 1./iters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in freq:\n",
    "    freq_ = freq[edge]\n",
    "\n",
    "    vals = np.array(list(freq_.values()))\n",
    "    p = (prob**edge)*((1.-prob)**((n*(n-1)/2)-edge))\n",
    "    print([edge, len(vals), vals.mean()/p, vals.mean(), vals.std(), vals.min(), vals.max(), p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(freq[7].items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def ratio(n):\n",
    "    print([n, np.sqrt(((n*(n-1)/2.) - np.log2(math.factorial(n)))*2)])\n",
    "\n",
    "for i in range(5,20):\n",
    "    ratio(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _neighbors(mat):\n",
    "    neigh = {'upper':{}, 'lower':{}}\n",
    "    n = mat.shape[0]\n",
    "    for i in range(n):\n",
    "        neigh['upper'][i] = []\n",
    "        for j in range(i):\n",
    "            if mat[j,i] == 1:\n",
    "                neigh['upper'][i].append(j)\n",
    "    for i in range(n):\n",
    "        neigh['lower'][i] = []\n",
    "        for j in range(i+1,n):\n",
    "            if mat[i,j] == 1:\n",
    "                neigh['lower'][i].append(j)\n",
    "    return neigh\n",
    "\n",
    "def _insert_to_down(merged, PPG, i_u, up):\n",
    "    Nu = PPG.shape[0]\n",
    "    # print('inserting index', i_u)\n",
    "    # print('merged:', merged)\n",
    "    # print('PPG:', PPG)\n",
    "\n",
    "    if i_u < up.shape[0] - 1:\n",
    "        after_ind = int(np.where(merged == up[i_u + 1])[0])\n",
    "    else:\n",
    "        after_ind = merged.shape[0]\n",
    "\n",
    "    if after_ind == i_u + 1:\n",
    "        # print('no space to move')\n",
    "        return\n",
    "\n",
    "    for i_d in range(i_u+1, after_ind):\n",
    "        q_u, q_d = 0, 0\n",
    "        \n",
    "        for k in range(i_d+1, after_ind):\n",
    "            q_d = q_d * (1. - PPG[merged[i_u]][merged[k]]) + PPG[merged[i_u]][merged[k]]\n",
    "\n",
    "        for k in range(i_u):\n",
    "            q_u = q_u * (1. - PPG[merged[k]][merged[i_d]]) + PPG[merged[k]][merged[i_d]]\n",
    "\n",
    "        q = q_u + q_d - (q_u * q_d)\n",
    "        q *= 1. - PPG[merged[i_u]][merged[i_d]]\n",
    "        if np.random.binomial(1, PPG[merged[i_u]][merged[i_d]] / (1. - q)) == 0:\n",
    "            break\n",
    "\n",
    "    # print('q_u:', q_u, 'q_d:', q_d, 'q:', q, 'p:', PPG[i_u][i_d])\n",
    "    if i_d > i_u + 1:\n",
    "        shift = merged[i_u+1:i_d]\n",
    "        merged_i_u = merged[i_u]\n",
    "        merged[i_u:i_d-1] = shift\n",
    "        merged[i_d-1] = merged_i_u\n",
    "\n",
    "    \n",
    "def get_permutation(selected):\n",
    "    return np.arange(selected.shape[0]) + selected.sum(1) - selected.sum(0)\n",
    "\n",
    "def PPG_merge(up, down, PPG):\n",
    "    Nu = up.shape[0]\n",
    "    Nd = down.shape[0]\n",
    "    \n",
    "    down += Nu\n",
    "    merged = np.concatenate([up, down])\n",
    "    # print('merge -> up:', up)\n",
    "    # print('down:', down)\n",
    "    # print('PPG:', PPG)\n",
    "\n",
    "    for i_u in reversed(range(Nu)):\n",
    "        _insert_to_down(merged, PPG, i_u, up)\n",
    "    return merged\n",
    "\n",
    "def PPG_sample(PPG):\n",
    "    n = PPG.shape[0]\n",
    "    mid = n // 2\n",
    "    # print('main:', n, mid)\n",
    "    if n == 1:\n",
    "        return np.array([0])\n",
    "    if n == 2:\n",
    "        if np.random.binomial(1,PPG[0,1]):\n",
    "            return np.array([1,0])\n",
    "        return np.array([0,1])\n",
    "    up = PPG_sample(PPG[:mid,:][:,:mid])\n",
    "    down = PPG_sample(PPG[mid:,:][:,mid:])\n",
    "    mat = PPG_merge(up, down, PPG)\n",
    "    # print('PPG:', PPG)\n",
    "    # print('mat:', mat)\n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 5\n",
    "prob = 0.5 * np.random.rand(n,n)\n",
    "PPG = prob * np.triu(np.ones((n,n)), 1)\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "def get_edges(args):\n",
    "    edges = 0\n",
    "    for i in range(len(args)):\n",
    "        for j in range(i+1, len(args)):\n",
    "            if args[i] > args[j]:\n",
    "                edges += 1\n",
    "    return edges\n",
    "    \n",
    "freq = defaultdict(lambda:defaultdict(lambda:0))\n",
    "\n",
    "iters = 1000\n",
    "for i in trange(iters):\n",
    "    positions = PPG_sample(PPG)\n",
    "    edges = get_edges(positions)\n",
    "    freq[edges][str(positions)] += 1./iters\n",
    "# print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in freq:\n",
    "    freq_ = freq[edge]\n",
    "\n",
    "    vals = np.array(list(freq_.values()))\n",
    "    p = (prob**edge)*((1.-prob)**((n*(n-1)/2)-edge))\n",
    "    print('edges:', edge, 'count:', len(vals), 'ratio:', vals.mean()/p, 'max to min:', vals.max()/vals.min())\n",
    "\n",
    "for edge in freq:\n",
    "    freq_ = list(freq[edge].items())\n",
    "    print(freq_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = np.array([[0,2,1],[2,1,0]])\n",
    "x = torch.FloatTensor(x)\n",
    "y = torch.LongTensor(y)\n",
    "x[torch.arange(x.shape[0]).unsqueeze(1).repeat((1,3)).flatten(), y.flatten()].view(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "list(permutations(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.45838083 0.06836439 0.22411404 0.31948567 0.10435878\n",
      "  0.11788836]\n",
      " [0.         0.         0.09808718 0.10696378 0.1844017  0.23926615\n",
      "  0.17164816]\n",
      " [0.         0.         0.         0.43323881 0.03733178 0.10648885\n",
      "  0.00583703]\n",
      " [0.         0.         0.         0.         0.14982386 0.04753529\n",
      "  0.22385135]\n",
      " [0.         0.         0.         0.         0.         0.49967702\n",
      "  0.05387474]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.4154824 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "0.09090277424436959\n",
      "[[ 0.          0.0048134  -0.04922486 -0.07947311 -0.1310312  -0.10017525\n",
      "  -0.10278114]\n",
      " [ 0.          0.         -0.05608157 -0.05929117 -0.10936817 -0.11831841\n",
      "  -0.10952495]\n",
      " [ 0.          0.          0.          0.00329471 -0.05183573 -0.08028392\n",
      "  -0.08866115]\n",
      " [ 0.          0.          0.          0.         -0.05868788 -0.06343596\n",
      "  -0.11066113]\n",
      " [ 0.          0.          0.          0.          0.         -0.04511577\n",
      "  -0.00848635]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.04860512]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n",
      "[[  0.           2.12864106  15.16900857   5.33627906   4.57147414\n",
      "   10.68433188   9.61327252]\n",
      " [  0.           0.          10.81195203  10.00120728   6.62607691\n",
      "    5.48103901   7.03072885]\n",
      " [  0.           0.           0.           2.27195148  27.35706294\n",
      "   10.2738389  172.29533558]\n",
      " [  0.           0.           0.           0.           7.32011588\n",
      "   21.73484828   5.68460723]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    2.49760067  18.65493009]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           2.94153431]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.        ]]\n",
      "[[ 0.         -1.89926678 -0.53186994 -0.41458429 -0.02803346 -0.01451415\n",
      "  -0.00297226]\n",
      " [ 0.          0.         -0.49181444 -0.46752726 -0.02296031 -0.01292729\n",
      "  -0.00235829]\n",
      " [ 0.          0.          0.         -1.80065589 -0.4685468  -0.23599576\n",
      "  -0.03053092]\n",
      " [ 0.          0.          0.          0.         -0.53061535 -0.35206366\n",
      "  -0.071056  ]\n",
      " [ 0.          0.          0.          0.          0.         -1.502401\n",
      "  -0.9635862 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -1.17611899]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "import PPG\n",
    "import numpy as np\n",
    "\n",
    "def compute_P(w):\n",
    "    P = 0\n",
    "    for perm in permutations(range(w.shape[0])):\n",
    "        e = PPG._get_edges(perm)\n",
    "        # print(P, e)\n",
    "        P += np.exp(np.log((e * w) + ((1. - e) * (1. - w))).sum())\n",
    "    return P\n",
    "\n",
    "\n",
    "n = 7\n",
    "w = 0.5 * np.random.rand(n, n) * np.triu(np.ones((n,n)), 1)\n",
    "print(w)\n",
    "P = compute_P(w)\n",
    "print(P)\n",
    "delta = 0.001 * w[w>0].min()\n",
    "w_prime = np.zeros_like(w)\n",
    "w_inv = np.zeros_like(w)\n",
    "w_inv_minus = np.zeros_like(w)\n",
    "\n",
    "for i in range(0, n):\n",
    "    for j in range(i+1, n):\n",
    "        w[i,j] += delta\n",
    "        w_prime[i,j] = (compute_P(w) - P) / delta\n",
    "        w[i,j] -= delta\n",
    "        w_inv[i,j] = 1./w[i,j] - w_prime[i,j]/P\n",
    "        w_inv_minus[i,j] = -1./(1. - w[i,j]) - w_prime[i,j]/P\n",
    "\n",
    "print(w_prime)\n",
    "print(w_inv)\n",
    "print(w_inv_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.05295104 -0.5415111  -0.87426491 -1.44144332 -1.10200436\n",
      "  -1.1306711 ]\n",
      " [ 0.          0.         -0.61694018 -0.65224815 -1.20313347 -1.30159291\n",
      "  -1.2048582 ]\n",
      " [ 0.          0.          0.          0.03624432 -0.57023269 -0.88318445\n",
      "  -0.97534038]\n",
      " [ 0.          0.          0.          0.         -0.6456115  -0.697844\n",
      "  -1.21735698]\n",
      " [ 0.          0.          0.          0.          0.         -0.49630791\n",
      "  -0.0933563 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.53469347]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_prime / P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0: 0.821562\n",
      "cost after iteration 100: 0.607784\n",
      "cost after iteration 200: 0.469293\n",
      "cost after iteration 300: 0.372667\n",
      "cost after iteration 400: 0.299758\n",
      "cost after iteration 500: 0.239811\n",
      "cost after iteration 600: 0.188136\n",
      "cost after iteration 700: 0.141315\n",
      "cost after iteration 800: 0.097574\n",
      "cost after iteration 900: 0.055483\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "def dRelu(x) :\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def dSigmoid(z) :\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    dZ = s * (1-s)\n",
    "    return dZ\n",
    "def Sigmoid(Z) :\n",
    "    return 1/ (1+np.exp(-Z))\n",
    "def Relu (Z) :\n",
    "    return np.maximum(0,Z)\n",
    "class dlnet:\n",
    "    def __init__(self, x, y) :\n",
    "        self.X=x\n",
    "        self.Y=y\n",
    "        self.Yh=np.zeros((1,self.Y.shape[1]))\n",
    "\n",
    "        self.L=2\n",
    "        self.dims = [9, 15, 1]\n",
    "\n",
    "        self.param = {}\n",
    "        self.ch = {}\n",
    "        self.grad = {}\n",
    "        self.loss = []\n",
    "        self.lr=0.003\n",
    "        self.sam = self.Y.shape[1]\n",
    "\n",
    "    def nInit(self) :\n",
    "        np.random.seed(1)\n",
    "        self.param['W1'] = np.random.randn(self.dims[1], \n",
    "        self.dims[0]) / np.sqrt(self.dims[0])\n",
    "        self.param['b1'] = np.zeros((self.dims[1], 1))\n",
    "        self.param['W2'] = np.random.randn(self.dims[2], \n",
    "        self.dims[1]) / np.sqrt(self.dims[1])\n",
    "        self.param['b2'] = np.zeros((self.dims[2], 1))\n",
    "        return\n",
    "    # squared_errors = (self.Yh - self.Y) ** 2\n",
    "    # self.loss= np.sum(squared_errors)\n",
    "\n",
    "    def forward(self) :\n",
    "        Z1 = self.param['W1'].dot(self.X) + self.param['b1']\n",
    "        A1 = Relu(Z1)\n",
    "        self.ch['Z1'],self.ch['A1']=Z1,A1\n",
    "        Z2 = self.param['W2'].dot(A1) + self.param['b2']\n",
    "        A2 = Sigmoid(Z2)\n",
    "        self.ch['Z2'],self.ch['A2']=Z2,A2\n",
    "        self.Yh=A2\n",
    "        loss=self.nloss(A2)\n",
    "        return self.Yh, loss\n",
    "\n",
    "    def nloss(self,Yh) :\n",
    "        loss = (1./self.sam) * (-np.dot(self.Y,np.log(Yh).T) - \n",
    "        np.dot(1-self.Y, np.log(1-Yh) .T))\n",
    "        return loss\n",
    "\n",
    "    def gd(self, X, Y, iter = 3000) :\n",
    "        np.random.seed(1)\n",
    "        self.nInit()\n",
    "        for i in range(0, iter) :\n",
    "            Yh, loss=self.forward()\n",
    "            self.backward()\n",
    "            if i % 100 == 0:\n",
    "                print (\"cost after iteration %i: %f\" %(i, loss))\n",
    "                self.loss.append(loss)\n",
    "        return        \n",
    "\n",
    "    def backward(self) :\n",
    "        dLoss_Yh = - (np.divide(self.Y, self.Yh) - np.divide(1 - \n",
    "        self.Y, 1 - self.Yh))\n",
    "        dLoss_Z2 = dLoss_Yh * dSigmoid(self.ch['Z2'])\n",
    "        dLoss_A1 = np.dot(self.param[\"W2\"].T,dLoss_Z2)\n",
    "        dLoss_W2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,\n",
    "        self.ch['A1'].T)\n",
    "        dLoss_b2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,\n",
    "        np.ones([dLoss_Z2.shape[1],1]))\n",
    "        dLoss_Z1 = dLoss_A1 * dRelu(self.ch['Z1'])\n",
    "        dLoss_A0 = np.dot(self.param[\"W1\"].T,dLoss_Z1)\n",
    "        dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)\n",
    "        dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,\n",
    "        np.ones([dLoss_Z1.shape[1],1]))\n",
    "\n",
    "        self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1\n",
    "        self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1\n",
    "        self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2\n",
    "        self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2\n",
    "\n",
    "x = np.random.randn(9, 100)\n",
    "y = np.random.randn(1,100)\n",
    "nn = dlnet(x,y)\n",
    "nn.gd(x, y, iter = 1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
