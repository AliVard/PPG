{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Mmetrics import *\n",
    "\n",
    "import LTR\n",
    "import datautil\n",
    "import permutationgraph\n",
    "import DTR\n",
    "import EEL\n",
    "import PPG\n",
    "import PL\n",
    "\n",
    "def df2ds(df_path):\n",
    "    with open(df_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    ds = df.to_dict(orient='list')\n",
    "    for k in ds:\n",
    "        ds[k] = np.array(ds[k])\n",
    "    ds['dlr'] = np.concatenate([np.zeros(1), np.where(np.diff(ds['qid'])==1)[0]+1, np.array([ds['qid'].shape[0]])]).astype(int)\n",
    "    return type('ltr', (object,), ds)\n",
    "\n",
    "\n",
    "def dict2ds(df_path):\n",
    "    with open(df_path, 'rb') as f:\n",
    "        ds = pickle.load(f)\n",
    "    return type('ltr', (object,), ds)\n",
    "\n",
    "ds2019 = df2ds('LTR2019.df')\n",
    "ds2020 = df2ds('LTR2020.df')\n",
    "sds2019 = dict2ds('s_LTR2019.df')\n",
    "sds2020 = dict2ds('s_LTR2020.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(ds, percent):\n",
    "    subsample_size = int((ds.dlr.shape[0]-1) * percent)\n",
    "    qids = np.random.choice(np.arange(ds.dlr.shape[0]-1), subsample_size)\n",
    "#     print(qids)\n",
    "    sds = {'lv':[], 'y_pred':[], 'qid':[], 'g':[], 'dlr':[0]}\n",
    "    for qid in qids:\n",
    "        s,e=ds.dlr[qid:qid+2]\n",
    "#         print(s,e)\n",
    "        sds['lv'].append(ds.lv[s:e])\n",
    "        sds['y_pred'].append(ds.y_pred[s:e])\n",
    "        sds['g'].append(ds.g[s:e])\n",
    "        sds['qid'].append(ds.qid[s:e])\n",
    "        sds['dlr'].append(sds['dlr'][-1]+e-s)\n",
    "    for k in ['lv', 'y_pred', 'g', 'qid']:\n",
    "        sds[k] = np.concatenate(sds[k])\n",
    "    sds['dlr'] = np.array(sds['dlr'])\n",
    "    return sds\n",
    "\n",
    "sds2019 = subsample(ds2019, 0.1)\n",
    "sds2020 = subsample(ds2020, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('s_LTR2019.df', 'wb') as f:\n",
    "    pickle.dump(sds2019, f)\n",
    "    \n",
    "with open('s_LTR2020.df', 'wb') as f:\n",
    "    pickle.dump(sds2020, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learn_one_PPG(qid, verbose, y_pred, g, dlr, epochs, lr, exposure, grade_levels, samples_cnt, sessions_cnt):\n",
    "    s, e = dlr[qid:qid+2]\n",
    "    y_pred_s, g_s, sorted_docs_s, dlr_s = \\\n",
    "        EEL.copy_sessions(y=y_pred[s:e], g=g[s:e], sorted_docs=y_pred[s:e].argsort()[::-1], sessions=sessions_cnt)\n",
    "    objective_ins = EEL.EEL(y_pred = y_pred_s, g = g_s, dlr = dlr_s, exposure=exposure, grade_levels = grade_levels)\n",
    "    learner = PPG.Learner(  PPG_mat=None, samples_cnt=samples_cnt, \n",
    "                                objective_ins=objective_ins, \n",
    "                                sorted_docs = sorted_docs_s, \n",
    "                                dlr = dlr_s,\n",
    "#                                 intra = np.arange(g_s.shape[0]),\n",
    "                                intra = g_s,\n",
    "                                inter = np.repeat(dlr_s[:-1], np.diff(dlr_s)))\n",
    "    vals = learner.fit(epochs, lr, verbose=verbose)\n",
    "    return vals\n",
    "\n",
    "def learn_all_PPG(y_pred, g, dlr, epochs, lr, exposure, grade_levels, samples_cnt, sessions_cnt):\n",
    "    sorted_docs = []\n",
    "    \n",
    "    for qid in trange(dlr.shape[0] - 1, leave=False):\n",
    "#     for qid in range(dlr.shape[0] - 1):\n",
    "        min_b = learn_one_PPG(qid, 0, y_pred, g, dlr, epochs, lr, exposure, grade_levels, samples_cnt, sessions_cnt)\n",
    "        sorted_docs.append(min_b)\n",
    "        \n",
    "\n",
    "    # print(ndcg_dtr(exposure, lv, np.concatenate(y_rerank), dlr, g, query_counts))\n",
    "    return sorted_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "res = learn_all_PPG(y_pred2020, ds2020.teg, ds2020.tedlr, epochs, 0.1, exposure=exposure2020,\n",
    "        grade_levels=5, samples_cnt=32, sessions_cnt=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(metric, qid, lv, g, dlr, output_permutation, exposure, sessions_cnt):\n",
    "    s, e = dlr[qid:qid+2]\n",
    "    permutation = output_permutation[qid]\n",
    "    lv_s, g_s, sorted_docs_s, dlr_s = \\\n",
    "        EEL.copy_sessions(y=lv[s:e], g=g[s:e], sorted_docs=lv[s:e].argsort()[::-1], sessions=sessions_cnt)\n",
    "    \n",
    "    if metric == 'EEL':\n",
    "        objective_ins = EEL.EEL(y_pred = lv_s, g = g_s, dlr = dlr_s, exposure=exposure, grade_levels = 2)\n",
    "    else:\n",
    "        objective_ins = DTR.DTR(y_pred = lv_s, g = g_s, dlr = dlr_s, exposure=exposure)\n",
    "        \n",
    "    \n",
    "    osl = e - s\n",
    "    argsort = lv[s:e].argsort()[::-1]\n",
    "    idcg = ((2.**lv[s:e][argsort][:min(osl,10)] - 1.) / (np.log2(2+np.arange(min(osl,10))))).sum()\n",
    "    ndcg = 0\n",
    "    for i in range(sessions_cnt):\n",
    "        ndcg += ((2.**lv[s:e][permutation[i*osl:(i+1)*osl]-(i*osl)][:min(osl,10)] - 1.) / (np.log2(2+np.arange(min(osl,10))))).sum() / idcg\n",
    "        \n",
    "    return objective_ins.eval(permutation), ndcg / sessions_cnt\n",
    " \n",
    "def evaluate_all(metric, lv, g, dlr, output_permutation, exposure, sessions_cnt):\n",
    "    eel_res, eer_res, eed_res, ndcgs = [], [], [], []\n",
    "    for qid in range(dlr.shape[0] - 1):\n",
    "        s,e = dlr[qid:qid+2]\n",
    "        if len(np.unique(g[s:e])) == 1:\n",
    "            continue\n",
    "        out1, ndcg = evaluate_one(metric, qid, lv, g, dlr, output_permutation, exposure, sessions_cnt)\n",
    "#         eel, eer, eed = out1\n",
    "        eel = out1\n",
    "        eel_res.append(eel)\n",
    "#         eer_res.append(eer)\n",
    "#         eed_res.append(eed)\n",
    "        ndcgs.append(ndcg)\n",
    "    return np.array(eel_res), np.array(ndcgs)\n",
    "#     return np.array(eel_res), np.array(eer_res), np.array(eed_res), np.array(ndcgs)\n",
    "\n",
    "def estimated_evaluate_one(qid, lv, g, dlr, output_permutation, exposure, sessions_cnt):\n",
    "    s, e = dlr[qid:qid+2]\n",
    "    permutation = output_permutation[qid]\n",
    "    lv_s, g_s, sorted_docs_s, dlr_s = \\\n",
    "        EEL.copy_sessions(y=lv[s:e], g=g[s:e], sorted_docs=lv[s:e].argsort()[::-1], sessions=sessions_cnt)\n",
    "    objective_ins = EEL.EEL(y_pred = lv_s, g = g_s, dlr = dlr_s, exposure=exposure, grade_levels = 5)\n",
    "    return objective_ins.eval_detailed(permutation)\n",
    " \n",
    "def estimated_evaluate_all(lv, g, dlr, output_permutation, exposure, sessions_cnt):\n",
    "    eel_res, eer_res, eed_res = [], [], []\n",
    "    for qid in range(dlr.shape[0] - 1):\n",
    "        s,e = ds.tedlr[qid:qid+2]\n",
    "        if len(np.unique(g[s:e])) == 1:\n",
    "            continue\n",
    "        eel, eer, eed = estimated_evaluate_one(qid, lv, g, dlr, output_permutation, exposure, sessions_cnt)\n",
    "        eel_res.append(eel)\n",
    "        eer_res.append(eer)\n",
    "        eed_res.append(eed)\n",
    "    return np.array(eel_res), np.array(eer_res), np.array(eed_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def read_results(directory = '_data/PPG'):\n",
    "    files = os.listdir(directory)\n",
    "    res = {}\n",
    "    for file in files:\n",
    "        if 'pkl' not in file:\n",
    "            continue\n",
    "        with open(f'{directory}/{file}', 'rb') as f:\n",
    "            res[file[:-12]] = pickle.load(f)\n",
    "    return res\n",
    "    \n",
    "res = read_results(directory = '_data/PPG/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_df_from_results_old(res):\n",
    "    exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "    exposure2019 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2019.tedlr).max()+2)])\n",
    "\n",
    "\n",
    "    dfl = []\n",
    "    for alg in tqdm(res, leave=True):\n",
    "        _res = res[alg]\n",
    "        alg_params = alg.split('_')\n",
    "    #     print(alg, '-'*30)\n",
    "        sessions_cnt = int(alg_params[3])\n",
    "        metric = alg_params[4]\n",
    "        learner = alg_params[1] + '_' + alg_params[0]\n",
    "        samples = int(alg_params[2])\n",
    "\n",
    "        for key in _res:\n",
    "            if '2019' in key:\n",
    "                year = 2019\n",
    "                ds = ds2019\n",
    "                ypred = y_pred2019\n",
    "                exposure = exposure2019\n",
    "            else:\n",
    "                year = 2020\n",
    "                ds = ds2020\n",
    "                ypred = y_pred2020\n",
    "                exposure = exposure2020\n",
    "\n",
    "            lr = key.split('_')[1]\n",
    "            eel_res, ndcg = evaluate_all(metric, ds.telv, ds.teg, ds.tedlr, _res[key], exposure, sessions_cnt=sessions_cnt)\n",
    "            dfl.append({'year':year, 'metric':metric, 'learner':learner, \n",
    "            'samples':samples, 'sessions':sessions_cnt, 'lr':lr, 'Fairness':eel_res.mean(), 'NDCG':ndcg.mean()})\n",
    "\n",
    "    return pd.DataFrame(dfl)\n",
    "\n",
    "\n",
    "\n",
    "def get_df_from_results(res):\n",
    "    exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.dlr).max()+2)])\n",
    "    exposure2019 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2019.dlr).max()+2)])\n",
    "\n",
    "\n",
    "    dfl = []\n",
    "    for alg in tqdm(res, leave=True):\n",
    "        _res = res[alg]\n",
    "        alg_params = alg.split('_')\n",
    "    #     print(alg, '-'*30)\n",
    "        sessions_cnt = int(alg_params[2])\n",
    "        metric = alg_params[3]\n",
    "        learner = alg_params[1] + '_' + alg_params[0]\n",
    "\n",
    "        for key in _res:\n",
    "            if '2019' in key:\n",
    "                year = 2019\n",
    "                ds = ds2019\n",
    "                exposure = exposure2019\n",
    "            else:\n",
    "                year = 2020\n",
    "                ds = ds2020\n",
    "                exposure = exposure2020\n",
    "\n",
    "            lr = key.split('_')[1]\n",
    "            samples = int(key.split('_')[2])\n",
    "            eel_res, ndcg = evaluate_all(metric, ds.lv, ds.g, ds.dlr, _res[key], exposure, sessions_cnt=sessions_cnt)\n",
    "            dfl.append({'year':year, 'metric':metric, 'learner':learner, \n",
    "            'samples':samples, 'sessions':sessions_cnt, 'lr':lr, 'Fairness':eel_res.mean(), 'NDCG':ndcg.mean()})\n",
    "\n",
    "    return pd.DataFrame(dfl)\n",
    "\n",
    "\n",
    "df = get_df_from_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['metric', 'sessions', 'year', 'learner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_df(df, sessions, metric, year):\n",
    "    return df.loc[(df.sessions==sessions) & (df.metric==metric) & (df.year == year), sorted(list(set(df.columns)-set(('metric', 'year'))))].sort_values(by=['samples', 'lr', 'learner'])\n",
    "\n",
    "# df_joint = df.merge(df_approx, on=['learner', 'lr', 'metric', 'samples', 'sessions', 'year'], how='left')\n",
    "for sessions in [1,2,4,8,16]:\n",
    "    print(refine_df(df, sessions, 'EEL', 2019).head(100))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exposure2020 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2020.tedlr).max()+2)])\n",
    "exposure2019 = np.array([1./np.log2(2+i) for i in range(1,np.diff(ds2019.tedlr).max()+2)])\n",
    "\n",
    "\n",
    "for alg in res:\n",
    "    _res = res[alg]\n",
    "    print(alg, '-'*30)\n",
    "    sessions_cnt = int(alg.split('_')[3])\n",
    "    metric = alg.split('_')[4]\n",
    "    for key in _res:\n",
    "        if '2019' in key:\n",
    "            ds = ds2019\n",
    "            ypred = y_pred2019\n",
    "            exposure = exposure2019\n",
    "        else:\n",
    "            ds = ds2020\n",
    "            ypred = y_pred2020\n",
    "            exposure = exposure2020\n",
    "            \n",
    "#         eel_res, eer_res, eed_res, ndcg = evaluate_all(ds.telv, ds.teg, ds.tedlr, _res[key], exposure, sessions_cnt=sessions_cnt)\n",
    "        eel_res, ndcg = evaluate_all(metric, ds.telv, ds.teg, ds.tedlr, _res[key], exposure, sessions_cnt=sessions_cnt)\n",
    "#         es_eel_res, es_eer_res, es_eed_res = estimated_evaluate_all(ypred, ds.teg, ds.tedlr, _res[key], exposure, sessions_cnt=20)\n",
    "    \n",
    "        print(f'{key}', eel_res.mean(), '(', ndcg.mean(), ')')\n",
    "#         print(f'{key}', eel_res.mean(), '(', es_eel_res.mean(), ')')\n",
    "#         print(f'\\t', ndcg.mean())\n",
    "#         print(f'\\t', eer_res.mean(), '(', es_eer_res.mean(), ')')\n",
    "#         print(f'\\t', eed_res.mean(), '(', es_eed_res.mean(), ')')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
